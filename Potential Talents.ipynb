{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to get multiple outputs in the same cell\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'\n",
    "\n",
    "#Code to ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019 C.T. Bauer College of Business Graduate (...</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Native English Teacher at EPIK (English Progra...</td>\n",
       "      <td>Kanada</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Aspiring Human Resources Professional</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>People Development Coordinator at Ryan</td>\n",
       "      <td>Denton, Texas</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Advisory Board Member at Celal Bayar University</td>\n",
       "      <td>İzmir, Türkiye</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
       "1   2  Native English Teacher at EPIK (English Progra...   \n",
       "2   3              Aspiring Human Resources Professional   \n",
       "3   4             People Development Coordinator at Ryan   \n",
       "4   5    Advisory Board Member at Celal Bayar University   \n",
       "\n",
       "                              location connection  fit  \n",
       "0                       Houston, Texas         85  NaN  \n",
       "1                               Kanada      500+   NaN  \n",
       "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
       "3                        Denton, Texas      500+   NaN  \n",
       "4                       İzmir, Türkiye      500+   NaN  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"C:\\Users\\97158\\Desktop\\Apziva\\Project 3- Potential Talents\\potential-talents - Aspiring human resources - seeking human resources.csv\")\n",
    "data.head()\n",
    "data1=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop('fit', axis=1)\n",
    "y=data['fit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk. stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2019 C.T. Bauer College of Business Graduate (Magna Cum Laude) and aspiring Human Resources professional'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title=list(X['job_title'])\n",
    "len(job_title)\n",
    "job_title[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019 c.t. bauer college of business graduate (magna cum laude) and aspiring human resources professional'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first job_title:  104\n"
     ]
    }
   ],
   "source": [
    "#Making all text lower case\n",
    "\n",
    "job_title_LowC=[word.lower() for word in job_title]\n",
    "job_title_LowC[0]\n",
    "print(\"Length of first job_title: \",len(job_title_LowC[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'c.t',\n",
       " '.',\n",
       " 'bauer',\n",
       " 'college',\n",
       " 'of',\n",
       " 'business',\n",
       " 'graduate',\n",
       " '(',\n",
       " 'magna',\n",
       " 'cum',\n",
       " 'laude',\n",
       " ')',\n",
       " 'and',\n",
       " 'aspiring',\n",
       " 'human',\n",
       " 'resources',\n",
       " 'professional']"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first job_title:  18\n"
     ]
    }
   ],
   "source": [
    "# tokenize doc in list(job_title)\n",
    "\n",
    "tokens= [word_tokenize(word) for word in job_title_LowC]\n",
    "tokens[0]\n",
    "print(\"Length of first job_title: \",len(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stop words in the document\n",
    "\n",
    "sw=stopwords.words('english')\n",
    "sw[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'c.t',\n",
       " '.',\n",
       " 'bauer',\n",
       " 'college',\n",
       " 'business',\n",
       " 'graduate',\n",
       " '(',\n",
       " 'magna',\n",
       " 'cum',\n",
       " 'laude',\n",
       " ')',\n",
       " 'aspiring',\n",
       " 'human',\n",
       " 'resources',\n",
       " 'professional']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first job_title:  16\n"
     ]
    }
   ],
   "source": [
    "# removing stopwords from each doc in list(job_title)\n",
    "# doc in list\n",
    "# words in doc\n",
    "\n",
    "tokens= [[word for word in doc if word not in sw] for doc in tokens]\n",
    "tokens[0]\n",
    "print(\"Length of first job_title: \",len(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation\n",
    "\n",
    "tokenizer=RegexpTokenizer(r'\\w+')\n",
    "tokens=[[\"\".join(tokenizer.tokenize(word)) for word in doc if len(tokenizer.tokenize(word))>0] for doc in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'ct',\n",
       " 'bauer',\n",
       " 'college',\n",
       " 'business',\n",
       " 'graduate',\n",
       " 'magna',\n",
       " 'cum',\n",
       " 'laude',\n",
       " 'aspiring',\n",
       " 'human',\n",
       " 'resources',\n",
       " 'professional']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first job_title:  13\n"
     ]
    }
   ],
   "source": [
    "tokens[0]\n",
    "print(\"Length of first job_title: \",len(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2019',\n",
       " 'ct',\n",
       " 'bauer',\n",
       " 'colleg',\n",
       " 'busi',\n",
       " 'graduat',\n",
       " 'magna',\n",
       " 'cum',\n",
       " 'laud',\n",
       " 'aspir',\n",
       " 'human',\n",
       " 'resourc',\n",
       " 'profession']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first job_title:  13\n"
     ]
    }
   ],
   "source": [
    "# Finding root words across documents\n",
    "# tokens in docs\n",
    "# words in docs\n",
    "\n",
    "\n",
    "tokens= [[porter.stem(word) for word in doc] for doc in tokens]\n",
    "\n",
    "tokens[0]\n",
    "print(\"Length of first job_title: \",len(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019 ct bauer colleg busi graduat magna cum laud aspir human resourc profession'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of all job_titles:  104\n"
     ]
    }
   ],
   "source": [
    "# Implementing Bag of words\n",
    "\n",
    "Clean_job_title=[\" \".join(word) for word in tokens]\n",
    "Clean_job_title[0]\n",
    "\n",
    "print(\"Length of all job_titles: \",len(Clean_job_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 35)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=CountVectorizer(binary=True, min_df=5)\n",
    "\n",
    "matrix=vect.fit_transform(Clean_job_title)\n",
    "\n",
    "matrix.shape  #Returns x and y where x is the total number of tekens and y is the count of binary tokens appearing it atleast 5 documents\n",
    "type(matrix)\n",
    "type(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above x and y shape pertains to x being the total number of tokens and y being the count of binary tokens appearing it atleast 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the number of unique words in the library\n",
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 35)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(<104x35 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 427 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Sparse Matrix into Array\n",
    "\n",
    "x_array=matrix\n",
    "g=np.squeeze(np.asarray(matrix))\n",
    "\n",
    "x_array.shape\n",
    "g.shape\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_sim=cosine_similarity(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.       , 0.       , 0.5547002, ..., 0.2773501, 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 1.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.5547002, 0.       , 1.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       ...,\n",
       "       [0.2773501, 0.       , 0.       , ..., 1.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ],\n",
       "       [0.       , 0.       , 0.       , ..., 0.       , 0.       ,\n",
       "        0.       ]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "cos_sim1=linear_kernel(matrix, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13.,  0.,  4., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  6.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 4.,  0.,  4., ...,  0.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 1.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recommendation Engine\n",
    "\n",
    "# def recommend(job_title, cosine_similarity=cos_sim):\n",
    "#     index = Clean_job_title.index(job_title)\n",
    "#     sim_score = list(enumerate(cos_sim[index]))\n",
    "#     sim_score = sorted(sim_score, key= lambda x: x[1], reverse=True)[1:11] #position 0 is the movie itself, thus exclude\n",
    "#     recommend_index = [i[0] for i in sim_score]\n",
    "#     rec_movie = data['job_title'].iloc[recommend_index]\n",
    "#     rec_score = [round(i[1],4) for i in sim_score]\n",
    "#     rec_table = pd.DataFrame(list(zip(rec_movie,rec_score)), columns=['Recommend Candidate','Similarity(0-1)'])\n",
    "#     return rec_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Recommendation Engine\n",
    "\n",
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# def find_similar(tfidf_matrix, index, top_n = 5):\n",
    "#     cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "#     related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "#     return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "\n",
    "    return dot(a, b)/(norm(a)*norm(b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Talents -- Project starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec/FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to clean the text in the dataframe\n",
    "\n",
    "\n",
    "# Function to to make all text lowercase\n",
    "def make_lowercase(text):\n",
    "    #lower=list(text)\n",
    "    lower= [doc.lower() for doc in text]\n",
    "    return lower\n",
    "\n",
    "#Removing numbers\n",
    "def remove_numbers(text):\n",
    "    remover=[[re.sub(r'\\d+','',word) for word in doc] for doc in text]\n",
    "    #remover=re.sub(r'\\d+','',text) # \\d+ removes numbers. re.sub finds and replaces that pattern in text. Takes 3 arguments \n",
    "    return remover\n",
    "\n",
    "#Removing punctuations\n",
    "regex=RegexpTokenizer(r'\\w+')\n",
    "def remove_punctuations(text):\n",
    "    removed_punct= [[\"\".join(regex.tokenize(word)) for word in doc if len(regex.tokenize(word))>0] for doc in text]  \n",
    "    return removed_punct\n",
    "\n",
    "#Removing Stop Words\n",
    "stop_words=stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    sw_remover=[[word for word in doc if word not in stop_words] for doc in text]\n",
    "    return sw_remover\n",
    "\n",
    "\n",
    "#Tokenizing texts\n",
    "def tokenize(text):\n",
    "    tokens=[word_tokenize(doc) for doc in text]\n",
    "    return tokens\n",
    "\n",
    "#Lemmatize Words\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "def root(text):\n",
    "    #flattened_text=[word for sublist in text for word in sublist]\n",
    "    #lemmatized_text=[lemmatizer.lemmatize(word) for word in flattened_text]\n",
    "    # text= [lemmatizer.lemmatize(word) for word_list in text for word in word_list]\n",
    "    lem_text= [[lemmatizer.lemmatize(word) for word in doc] for doc in text]\n",
    "    return lem_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Compiling all the above def functions to preprocess all in one step\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    text=make_lowercase(text)\n",
    "    text=tokenize(text)\n",
    "    text=remove_stopwords(text)\n",
    "    text=remove_punctuations(text)\n",
    "    text=remove_numbers(text)\n",
    "    text=root(text)\n",
    "  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['job_title']=preprocess(data['job_title'])\n",
    "data['location']=preprocess(data['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['job_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[, ct, bauer, college, business, graduate, mag...</td>\n",
       "      <td>[houston, texas]</td>\n",
       "      <td>85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[native, english, teacher, epik, english, prog...</td>\n",
       "      <td>[kanada]</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[aspiring, human, resource, professional]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[people, development, coordinator, ryan]</td>\n",
       "      <td>[denton, texas]</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[advisory, board, member, celal, bayar, univer...</td>\n",
       "      <td>[izmir, türkiye]</td>\n",
       "      <td>500+</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          job_title  \\\n",
       "0   1  [, ct, bauer, college, business, graduate, mag...   \n",
       "1   2  [native, english, teacher, epik, english, prog...   \n",
       "2   3          [aspiring, human, resource, professional]   \n",
       "3   4           [people, development, coordinator, ryan]   \n",
       "4   5  [advisory, board, member, celal, bayar, univer...   \n",
       "\n",
       "                                 location connection  fit  \n",
       "0                        [houston, texas]         85  NaN  \n",
       "1                                [kanada]      500+   NaN  \n",
       "2  [raleighdurham, north, carolina, area]         44  NaN  \n",
       "3                         [denton, texas]      500+   NaN  \n",
       "4                        [izmir, türkiye]      500+   NaN  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('human', 63),\n",
       " ('resource', 63),\n",
       " ('aspiring', 35),\n",
       " ('professional', 21),\n",
       " ('student', 16),\n",
       " ('seeking', 15),\n",
       " ('college', 14),\n",
       " ('generalist', 14),\n",
       " ('university', 12),\n",
       " ('specialist', 12)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_tokens=[word for t in data['job_title'] for word in t]\n",
    "counts=Counter(flat_tokens)\n",
    "\n",
    "counts.most_common(10)\n",
    "len(counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [, ct, bauer, college, business, graduate, mag...\n",
       "1      [native, english, teacher, epik, english, prog...\n",
       "2              [aspiring, human, resource, professional]\n",
       "3               [people, development, coordinator, ryan]\n",
       "4      [advisory, board, member, celal, bayar, univer...\n",
       "                             ...                        \n",
       "99     [aspiring, human, resource, manager, graduatin...\n",
       "100               [human, resource, generalist, loparex]\n",
       "101        [business, intelligence, analytics, traveler]\n",
       "102                               [always, set, success]\n",
       "103      [director, administration, excellence, logging]\n",
       "Name: job_title, Length: 104, dtype: object"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=data.job_title\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings = {}\n",
    "with open(r\"C:\\Users\\97158\\Downloads\\glove.6B\\glove.6B.100d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = [float(val) for val in values[1:]]\n",
    "        embeddings[word] = vector\n",
    "        #coefs = np.asarray(values[1:], dtype='float32')\n",
    "        #embedding_index[word] = coefs\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "    glove_embed=embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sentence_embedding(sentence, embeddings):\n",
    "    words = sentence\n",
    "    vectorized_words = [embeddings.get(word, np.zeros(100)) for word in words]\n",
    "    sentence_emb = np.mean(vectorized_words, axis=0)\n",
    "    return sentence_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def query_embedding(Query, embeddings):\n",
    "    words = Query.split()\n",
    "    vectorized_words = [embeddings.get(word, np.zeros(100)) for word in words]\n",
    "    sentence_emb = np.mean(vectorized_words, axis=0)\n",
    "    return sentence_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query= \"Aspiring human resources\"\n",
    "\n",
    "\n",
    "def Glove_Similarity_score_Cal(df,Query,embedding):                     \n",
    "    score=[]\n",
    "    for index,row in df.iterrows():\n",
    "        sentence=row['job_title']\n",
    "\n",
    "        # Compute embeddings\n",
    "        sentence_emb = sentence_embedding(sentence, embedding)\n",
    "        sentence_emb = sentence_emb.reshape(1,-1) \n",
    "        \n",
    "        Query_emb = query_embedding(Query, embedding)\n",
    "        Query_emb = Query_emb.reshape(1,-1) \n",
    "        \n",
    "                                        \n",
    "        #Calculating cosine_similarity      \n",
    "        Similarity_score=cosine_similarity(sentence_emb,Query_emb)\n",
    "        score.append(Similarity_score[0][0])\n",
    "    \n",
    "    df['fit']=score\n",
    "    df=df.sort_values(by='fit', ascending=False)\n",
    "    GloVe_df=df.drop_duplicates(subset=['job_title'])\n",
    "\n",
    "    return GloVe_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>[seeking, human, resource, opportunity]</td>\n",
       "      <td>[chicago, illinois]</td>\n",
       "      <td>390</td>\n",
       "      <td>0.887072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>[human, resource, management, major]</td>\n",
       "      <td>[milpitas, california]</td>\n",
       "      <td>18</td>\n",
       "      <td>0.880439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>[seeking, human, resource, position]</td>\n",
       "      <td>[la, vega, nevada, area]</td>\n",
       "      <td>48</td>\n",
       "      <td>0.879327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>[human, resource, professional]</td>\n",
       "      <td>[greater, boston, area]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>[aspiring, human, resource, manager, seeking, ...</td>\n",
       "      <td>[houston, texas, area]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.855437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>[seeking, human, resource, hris, generalist, p...</td>\n",
       "      <td>[greater, philadelphia, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.851628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>[human, resource, conflict, management, policy...</td>\n",
       "      <td>[dallasfort, worth, area]</td>\n",
       "      <td>409</td>\n",
       "      <td>0.843459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>[human, resource, generalist, loparex]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.837878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>[director, human, resource, ey]</td>\n",
       "      <td>[greater, atlanta, area]</td>\n",
       "      <td>349</td>\n",
       "      <td>0.837395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>[senior, human, resource, business, partner, h...</td>\n",
       "      <td>[chattanooga, tennessee, area]</td>\n",
       "      <td>455</td>\n",
       "      <td>0.821940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>[human, resource, generalist, scottmadden, inc]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.819823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>[aspiring, human, resource, specialist]</td>\n",
       "      <td>[greater, new, york, city, area]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.811970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>84</td>\n",
       "      <td>[human, resource, professional, world, leader,...</td>\n",
       "      <td>[highland, california]</td>\n",
       "      <td>50</td>\n",
       "      <td>0.795846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>[director, human, resource, north, america, gr...</td>\n",
       "      <td>[greater, grand, rapid, michigan, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.793044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>[human, resource, specialist, luxottica]</td>\n",
       "      <td>[greater, new, york, city, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.792844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          job_title  \\\n",
       "29    30            [seeking, human, resource, opportunity]   \n",
       "87    88               [human, resource, management, major]   \n",
       "98    99               [seeking, human, resource, position]   \n",
       "73    74                    [human, resource, professional]   \n",
       "72    73  [aspiring, human, resource, manager, seeking, ...   \n",
       "52    53  [seeking, human, resource, hris, generalist, p...   \n",
       "76    77  [human, resource, conflict, management, policy...   \n",
       "100  101             [human, resource, generalist, loparex]   \n",
       "88    89                    [director, human, resource, ey]   \n",
       "80    81  [senior, human, resource, business, partner, h...   \n",
       "70    71    [human, resource, generalist, scottmadden, inc]   \n",
       "35    36            [aspiring, human, resource, specialist]   \n",
       "83    84  [human, resource, professional, world, leader,...   \n",
       "68    69  [director, human, resource, north, america, gr...   \n",
       "67    68           [human, resource, specialist, luxottica]   \n",
       "\n",
       "                                    location connection       fit  \n",
       "29                       [chicago, illinois]        390  0.887072  \n",
       "87                    [milpitas, california]         18  0.880439  \n",
       "98                  [la, vega, nevada, area]         48  0.879327  \n",
       "73                   [greater, boston, area]         16  0.868421  \n",
       "72                    [houston, texas, area]          7  0.855437  \n",
       "52             [greater, philadelphia, area]      500+   0.851628  \n",
       "76                 [dallasfort, worth, area]        409  0.843459  \n",
       "100   [raleighdurham, north, carolina, area]      500+   0.837878  \n",
       "88                  [greater, atlanta, area]        349  0.837395  \n",
       "80            [chattanooga, tennessee, area]        455  0.821940  \n",
       "70    [raleighdurham, north, carolina, area]      500+   0.819823  \n",
       "35          [greater, new, york, city, area]          1  0.811970  \n",
       "83                    [highland, california]         50  0.795846  \n",
       "68   [greater, grand, rapid, michigan, area]      500+   0.793044  \n",
       "67          [greater, new, york, city, area]      500+   0.792844  "
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Glove_Similarity_score_Cal(data,Query,glove_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec & FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=67, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# Training Skip Gram Model for Word2Vec\n",
    "\n",
    "skipgram2=Word2Vec(x,vector_size=100,window=2,min_count=2,sg=1)\n",
    "\n",
    "#Window=3 means model will consider 1 word at a time while training\n",
    "#min_count=2 means that words that appear at least twice will be included in the vocab.\n",
    "\n",
    "print(skipgram2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText<vocab=67, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "# Training FastText Model\n",
    "\n",
    "fast_n= FastText(x, vector_size=100, window=2, min_count=2, min_n=1, max_n=2, sg=1)\n",
    "\n",
    "# A large window size allows the model to look at more context\n",
    "\n",
    "print(fast_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marketing', 0.2013355791568756),\n",
       " ('manager', 0.19968749582767487),\n",
       " ('hr', 0.19095101952552795),\n",
       " ('bauer', 0.18173237144947052),\n",
       " ('humber', 0.16728708148002625),\n",
       " ('english', 0.16227439045906067),\n",
       " ('america', 0.1455521583557129),\n",
       " ('development', 0.14216649532318115),\n",
       " ('svp', 0.12910626828670502),\n",
       " ('ct', 0.12734930217266083)]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Embeddings-Similarity\n",
    "\n",
    "skipgram2.wv.most_similar(positive=['human'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('graduate', 0.24318785965442657),\n",
       " ('university', 0.22627538442611694),\n",
       " ('information', 0.21257394552230835),\n",
       " ('houston', 0.2099032998085022),\n",
       " ('management', 0.20342735946178436),\n",
       " ('sphr', 0.19163864850997925),\n",
       " ('teacher', 0.17736878991127014),\n",
       " ('engie', 0.1664290726184845),\n",
       " ('chapman', 0.1503874808549881),\n",
       " ('major', 0.1497708559036255)]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram2.wv.most_similar(positive=['resource'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chapman', 0.9678015112876892),\n",
       " ('information', 0.962955117225647),\n",
       " ('communication', 0.9569567441940308),\n",
       " ('marketing', 0.9505852460861206),\n",
       " ('management', 0.9505007863044739),\n",
       " ('coordinator', 0.9504880309104919),\n",
       " ('senior', 0.94764643907547),\n",
       " ('manager', 0.9463614225387573),\n",
       " ('aspiring', 0.9455015659332275),\n",
       " ('english', 0.9403043389320374)]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similar words using Fast Text\n",
    "\n",
    "fast_n.wv.most_similar(positive=['human'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('intercontinental', 0.9630917310714722),\n",
       " ('director', 0.9596865177154541),\n",
       " ('senior', 0.9572448134422302),\n",
       " ('generalist', 0.9566609859466553),\n",
       " ('engie', 0.9560286998748779),\n",
       " ('internship', 0.9540278911590576),\n",
       " ('retail', 0.9537052512168884),\n",
       " ('university', 0.9535289406776428),\n",
       " ('professional', 0.9532554745674133),\n",
       " ('development', 0.9530006051063538)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_n.wv.most_similar(positive=['resource'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to take average of all vectors of each word in job_title so that each job_title can represent (1,100) dimensional vector\n",
    "\n",
    "def get_mean_vector(Model, doc):\n",
    "\n",
    "    words=[word for word in doc if word in Model.wv.index_to_key]\n",
    "    if len(words)>=1:\n",
    "        return np.mean(Model.wv[words], axis=0)\n",
    "    else:\n",
    "        return np.array([0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to take average of a vector for each word in job_title so that each job_title can represt (1,100) dimensinal vector\n",
    "# def query_mean(Model, query):\n",
    "\n",
    "#     words=[word for word in query if word in Model.wv.index_to_key]\n",
    "#     if len(query)>=1:\n",
    "#         return np.mean(Model.wv[query], axis=0)\n",
    "#     else:\n",
    "#         return np.array([0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>8.835207e-07</td>\n",
       "      <td>0.004786</td>\n",
       "      <td>-0.002561</td>\n",
       "      <td>-0.002443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.002226</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>-0.001268</td>\n",
       "      <td>-0.000932</td>\n",
       "      <td>-0.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002065</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>-0.002731</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>-0.001379</td>\n",
       "      <td>-0.003207</td>\n",
       "      <td>-3.798845e-03</td>\n",
       "      <td>-0.001079</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>-0.002693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>-0.001035</td>\n",
       "      <td>-0.000679</td>\n",
       "      <td>-0.000289</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>-0.002439</td>\n",
       "      <td>-0.001851</td>\n",
       "      <td>-0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.004331</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.002981</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>1.598239e-03</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>-0.002718</td>\n",
       "      <td>-0.003187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>0.006241</td>\n",
       "      <td>0.003303</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-0.000977</td>\n",
       "      <td>-0.000495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.004154</td>\n",
       "      <td>-0.003903</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>-0.002202</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>1.790616e-03</td>\n",
       "      <td>0.004054</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-0.002236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>-0.006257</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.005798</td>\n",
       "      <td>-0.001003</td>\n",
       "      <td>-0.002423</td>\n",
       "      <td>0.008033</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.002589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002185</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.002807</td>\n",
       "      <td>-0.002082</td>\n",
       "      <td>-0.004179</td>\n",
       "      <td>5.893023e-04</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002409</td>\n",
       "      <td>0.002446</td>\n",
       "      <td>-0.000637</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.005130</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>-0.006044</td>\n",
       "      <td>0.002324</td>\n",
       "      <td>-0.001782</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5             6   \\\n",
       "0  0.001345  0.001193  0.002517  0.002969  0.000618 -0.002404  8.835207e-07   \n",
       "1 -0.002065 -0.001675 -0.002731  0.000154 -0.001379 -0.003207 -3.798845e-03   \n",
       "2 -0.004331  0.003942  0.000803  0.002981  0.002465 -0.002866  1.598239e-03   \n",
       "3 -0.004154 -0.003903  0.003119 -0.003799 -0.002202 -0.000479  1.790616e-03   \n",
       "4 -0.002185 -0.000136  0.000374  0.002807 -0.002082 -0.004179  5.893023e-04   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0  0.004786 -0.002561 -0.002443  ...  0.002675 -0.000027  0.000926  0.001033   \n",
       "1 -0.001079 -0.000762 -0.002693  ...  0.002442  0.002448  0.002155 -0.001035   \n",
       "2  0.006624 -0.002718 -0.003187  ... -0.001884  0.000751  0.003638 -0.003435   \n",
       "3  0.004054 -0.000794 -0.002236  ...  0.004498 -0.006257  0.000074 -0.005798   \n",
       "4  0.002730 -0.001350 -0.003703  ...  0.002409  0.002446 -0.000637  0.002519   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.001884  0.002226  0.000260 -0.001268 -0.000932 -0.000929  \n",
       "1 -0.000679 -0.000289  0.001786 -0.002439 -0.001851 -0.000201  \n",
       "2  0.006241  0.003303  0.001994 -0.003863 -0.000977 -0.000495  \n",
       "3 -0.001003 -0.002423  0.008033  0.001198  0.004467  0.002589  \n",
       "4  0.005130 -0.001134 -0.006044  0.002324 -0.001782  0.000846  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(104, 100)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Job_title to Vectors using Skip Gram Model\n",
    "\n",
    "sg_mean= [get_mean_vector(skipgram2,i) for i in data.job_title]\n",
    "skipg_mean=pd.DataFrame(sg_mean)\n",
    "\n",
    "skipg_mean.head()\n",
    "skipg_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003896</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>-0.010979</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>-0.002789</td>\n",
       "      <td>-0.003376</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>-0.001066</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>-0.003084</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>-0.006140</td>\n",
       "      <td>0.002993</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>-0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.006597</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>-0.011625</td>\n",
       "      <td>0.003552</td>\n",
       "      <td>-0.003070</td>\n",
       "      <td>-0.003396</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>-0.003348</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.002382</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.004785</td>\n",
       "      <td>-0.005924</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>-0.003362</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.006791</td>\n",
       "      <td>0.007601</td>\n",
       "      <td>-0.012338</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>-0.002167</td>\n",
       "      <td>-0.004100</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>-0.003667</td>\n",
       "      <td>0.005863</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>-0.006235</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>-0.003245</td>\n",
       "      <td>-0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006577</td>\n",
       "      <td>0.007967</td>\n",
       "      <td>-0.013006</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>-0.002314</td>\n",
       "      <td>-0.004006</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>-0.000570</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>-0.003293</td>\n",
       "      <td>0.006551</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.004796</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>-0.002344</td>\n",
       "      <td>-0.000497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.005842</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>-0.010355</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>-0.003086</td>\n",
       "      <td>-0.003537</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>-0.000331</td>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>-0.003087</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.004737</td>\n",
       "      <td>-0.005045</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>-0.002246</td>\n",
       "      <td>0.000787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -0.003896  0.006873 -0.010979  0.003486 -0.002789 -0.003376  0.001189   \n",
       "1 -0.006597  0.007489 -0.011625  0.003552 -0.003070 -0.003396  0.002458   \n",
       "2 -0.006791  0.007601 -0.012338  0.002884 -0.002167 -0.004100  0.000654   \n",
       "3 -0.006577  0.007967 -0.013006  0.002669 -0.002314 -0.004006  0.003616   \n",
       "4 -0.005842  0.006454 -0.010355  0.003123 -0.003086 -0.003537  0.002135   \n",
       "\n",
       "         7         8         9   ...        90        91        92        93  \\\n",
       "0 -0.001066  0.005136  0.000434  ...  0.008723 -0.003084  0.004119  0.002535   \n",
       "1 -0.000059  0.006260  0.000961  ...  0.009276 -0.003348  0.006073  0.002382   \n",
       "2 -0.000045  0.006197  0.000242  ...  0.009625 -0.003667  0.005863  0.002772   \n",
       "3 -0.000570  0.006656 -0.000872  ...  0.009490 -0.003293  0.006551  0.003259   \n",
       "4 -0.000331  0.004739  0.000866  ...  0.009224 -0.003087  0.005941  0.000973   \n",
       "\n",
       "         94        95        96        97        98        99  \n",
       "0  0.001942  0.004532 -0.006140  0.002993 -0.002063 -0.000105  \n",
       "1  0.002381  0.004785 -0.005924  0.002485 -0.003362  0.000035  \n",
       "2  0.003214  0.006968 -0.006235  0.002630 -0.003245 -0.000507  \n",
       "3  0.002938  0.004796 -0.007104  0.003625 -0.002344 -0.000497  \n",
       "4  0.002310  0.004737 -0.005045  0.002923 -0.002246  0.000787  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(104, 100)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Job_title to Vectors using FasttText Model\n",
    "\n",
    "ft_mean= [get_mean_vector(fast_n,i) for i in data.job_title]\n",
    "\n",
    "fastT_mean=pd.DataFrame(ft_mean)\n",
    "\n",
    "fastT_mean.head()\n",
    "fastT_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 100)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(104, 100)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_text_vectors=fastT_mean.to_numpy()\n",
    "fast_text_vectors.shape\n",
    "\n",
    "skip_gram_vectors=skipg_mean.to_numpy()\n",
    "skip_gram_vectors.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity Function\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess User's input query\n",
    "\n",
    "\n",
    "# def Input_preprocessor(query, model):\n",
    "#     query= preprocess(query)\n",
    "#     print(\"Query from Preporocess: \", query)\n",
    "#     input_vec=query_mean(model, query)\n",
    "#     print(\"mean vec after prerpcess: \", input_vec)\n",
    "#     return input_vec\n",
    "\n",
    "def query_mean(model, query):\n",
    "    if len(query)>=1:\n",
    "        return np.mean(model.wv[query], axis=0)\n",
    "    else:\n",
    "        return np.array([0]*100)\n",
    "                \n",
    "def Input_preprocessor(query,model):\n",
    "    input_vec=query_mean(model, query)\n",
    "    return input_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(query_reshaped,row_vector_reshaped):\n",
    "    return np.dot(row_vector_reshaped, query_reshaped) / (np.linalg.norm(row_vector_reshaped) * np.linalg.norm(query_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Funciton to return n similar results\n",
    "\n",
    "\n",
    "# # Query= User's input\n",
    "# # vec= Skip gram vectors\n",
    "# # df= orifinal data\n",
    "# # Model= trained model\n",
    "\n",
    "# def Top_n(query, model, vectors,data):\n",
    "#     print(\"query1 \",query)\n",
    "#     query = Input_preprocessor(query, model)\n",
    "#     print(\"query2: \",query)\n",
    "#     x = []\n",
    "#     for i in range(len(vectors)):\n",
    "#         # Reshape both query and row_vector to ensure compatible shapes for cosine similarity\n",
    "#         query_reshaped = query[0].reshape(1,-1)   #Because it is a single sample and not single feature\n",
    "#         row_vector_reshaped = vectors[i].reshape(-1,1)   #Because it is a single feature with multiple samples\n",
    "        \n",
    "#         cos_sim(query_reshaped,row_vector_reshaped)\n",
    "        \n",
    "#         # # Print the cosine similarities\n",
    "#         # for i, sim in enumerate(similarities):\n",
    "#         #     print(f\"Cosine Similarity with element {i + 1}: {sim[0]}\")    \n",
    "#         x.append(cos_sim)\n",
    "#         print(\"X: \",x)\n",
    "#     temp = list(x)  \n",
    "    \n",
    "\n",
    "#     sort=sorted(range(len(x)),key=lambda sub: x[sub])[-10:]    \n",
    "#     sim=[temp[i] for i in reversed(sort)]\n",
    "#     print(sim)\n",
    "\n",
    "#     #Getting index of the top 10\n",
    "#     Ind=[]\n",
    "#     for i in reversed(sort):\n",
    "#         Ind.append(i)\n",
    "\n",
    "#     return  data.iloc[Ind,[1,2,3,4]], sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton to return n similar results\n",
    "\n",
    "\n",
    "# Query= User's input\n",
    "# model= Vector embedding model\n",
    "# vectors= embedded vectors\n",
    "# data= original dataframe\n",
    "\n",
    "def Top_n(query, model, vectors,data):\n",
    "\n",
    "    #Q_df=pd.DataFrame({\"Query\": [query]})\n",
    "    Q_df=pd.DataFrame( [query])\n",
    "    #Q_df=[query]\n",
    "    query = Input_preprocessor(Q_df, model)\n",
    "    \n",
    "    x = []\n",
    "    for i in range(len(vectors)):\n",
    "        x.append(cos_sim(query,vectors[i]))\n",
    "        \n",
    "    print('this is x: ',x)\n",
    "    data['fit']=x\n",
    "    data=data.sort_values(by='fit', ascending=False)\n",
    "    data=data.drop_duplicates(subset=['job_title'])\n",
    "   \n",
    "    return  data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aspiring human resources</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          0\n",
       "0  Aspiring human resources"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query= \"Aspiring human resources\"\n",
    "\n",
    "Q_df=pd.DataFrame( [Query])\n",
    "Q_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query= \"Aspiring human resources\"\n",
    "\n",
    "Q_df=pd.DataFrame( [Query])\n",
    "    #Q_df=[query]\n",
    "query = Input_preprocessor(Q_df, fast_n)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is x:  [-0.0100288205, -0.02408786, -0.012003587, -0.01741786, -0.015384086, -0.01186889, -0.011148738, -0.014749579, -0.011148738, -0.02407155, -0.015054408, 0.0036662035, -0.002655471, -0.0100288205, -0.0100288205, -0.02408786, -0.012003587, -0.01741786, -0.0100288205, -0.02408786, -0.012003587, -0.01741786, -0.015384086, -0.01186889, -0.011148738, -0.014749579, -0.017895203, -0.009924578, -0.017895203, -0.009924578, -0.0100288205, -0.02408786, -0.012003587, -0.01741786, -0.015384086, -0.01186889, -0.011148738, -0.014749579, -0.011148738, -0.02407155, -0.015054408, 0.0036662035, -0.002655471, -0.0100288205, -0.02408786, -0.012003587, -0.01741786, -0.015384086, -0.01186889, -0.011148738, -0.014749579, -0.011148738, -0.02407155, -0.015054408, 0.0036662035, -0.002655471, -0.0100288205, -0.012003587, -0.01741786, -0.01186889, -0.014749579, -0.02407155, -0.015054408, 0.0036662035, -0.002655471, -0.0041042734, 0.0010898124, -0.006825996, -0.021284923, -0.013726897, -0.009204473, -0.016403036, -0.018805003, -0.0071539287, -0.009892631, -0.012003587, -0.0093576955, -0.009204473, -0.022266177, -0.025521576, -0.016041072, -0.01765071, -0.021513684, -0.015365595, nan, -0.01623555, -0.026876656, -0.016652279, -0.014454099, nan, -0.026876656, -0.0083864285, nan, -0.009924578, -0.010120908, -0.0027393359, -0.012003587, 0.005407755, -0.015732648, -0.017355397, -0.009204473, -0.015307839, nan, -0.020470949]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Student</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>SVP, CHRO, Marketing &amp; Communications, CSR Off...</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.003666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>Human Resources, Staffing and Recruiting Profe...</td>\n",
       "      <td>Jackson, Mississippi Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.001090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Human Resources Coordinator at InterContinenta...</td>\n",
       "      <td>Atlanta, Georgia</td>\n",
       "      <td>500+</td>\n",
       "      <td>-0.002655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Student at Indiana University Kokomo - Busines...</td>\n",
       "      <td>Lafayette, Indiana</td>\n",
       "      <td>19</td>\n",
       "      <td>-0.002739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>Experienced Retail Manager and aspiring Human ...</td>\n",
       "      <td>Austin, Texas Area</td>\n",
       "      <td>57</td>\n",
       "      <td>-0.004104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>Human Resources Specialist at Luxottica</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>-0.006826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>Human Resources Professional</td>\n",
       "      <td>Greater Boston Area</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.007154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>92</td>\n",
       "      <td>Seeking employment opportunities within Custom...</td>\n",
       "      <td>Torrance, California</td>\n",
       "      <td>64</td>\n",
       "      <td>-0.008386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>-0.009204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                          job_title  \\\n",
       "97  98                                            Student   \n",
       "11  12  SVP, CHRO, Marketing & Communications, CSR Off...   \n",
       "66  67  Human Resources, Staffing and Recruiting Profe...   \n",
       "12  13  Human Resources Coordinator at InterContinenta...   \n",
       "95  96  Student at Indiana University Kokomo - Busines...   \n",
       "65  66  Experienced Retail Manager and aspiring Human ...   \n",
       "67  68            Human Resources Specialist at Luxottica   \n",
       "73  74                       Human Resources Professional   \n",
       "91  92  Seeking employment opportunities within Custom...   \n",
       "77  78             Human Resources Generalist at Schwan's   \n",
       "\n",
       "                       location connection       fit  \n",
       "97          Houston, Texas Area          4  0.005408  \n",
       "11          Houston, Texas Area      500+   0.003666  \n",
       "66    Jackson, Mississippi Area      500+   0.001090  \n",
       "12             Atlanta, Georgia      500+  -0.002655  \n",
       "95           Lafayette, Indiana         19 -0.002739  \n",
       "65           Austin, Texas Area         57 -0.004104  \n",
       "67   Greater New York City Area      500+  -0.006826  \n",
       "73          Greater Boston Area         16 -0.007154  \n",
       "91         Torrance, California         64 -0.008386  \n",
       "77  Amerika Birleşik Devletleri      500+  -0.009204  "
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query= \"Aspiring human resources\"\n",
    "\n",
    "\n",
    "\n",
    "sim_score=Top_n(Query, fast_n,fast_text_vectors,data1)\n",
    "sim_score.head(10)\n",
    "# data['fit']=sim_score\n",
    "# data=data.sort_values(by='fit', ascending=False)\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is x:  [0.3874439, 0.09169324, 0.44518843, -0.044438798, 0.20438199, 0.47797257, 0.39035848, 0.02834687, 0.39035848, 0.4067095, 0.24885643, 0.17605889, 0.42625052, 0.3874439, 0.3874439, 0.09169324, 0.44518843, -0.044438798, 0.3874439, 0.09169324, 0.44518843, -0.044438798, 0.20438199, 0.47797257, 0.39035848, 0.02834687, 0.46537662, 0.50639397, 0.46537662, 0.50639397, 0.3874439, 0.09169324, 0.44518843, -0.044438798, 0.20438199, 0.47797257, 0.39035848, 0.02834687, 0.39035848, 0.4067095, 0.24885643, 0.17605889, 0.42625052, 0.3874439, 0.09169324, 0.44518843, -0.044438798, 0.20438199, 0.47797257, 0.39035848, 0.02834687, 0.39035848, 0.4067095, 0.24885643, 0.17605889, 0.42625052, 0.3874439, 0.44518843, -0.044438798, 0.47797257, 0.02834687, 0.4067095, 0.24885643, 0.17605889, 0.42625052, 0.35894284, 0.4737077, 0.5867843, 0.43014094, 0.42044377, 0.61696094, 0.5239723, 0.56880647, 0.5267421, 0.33205405, 0.44518843, 0.6677327, 0.61696094, 0.5447279, 0.1374918, 0.53285426, 0.41966096, 0.015788045, 0.49041343, nan, 0.1367619, 0.22627541, 0.62760264, 0.5765931, nan, 0.22627541, -0.029618243, nan, 0.50639397, 0.16012089, 0.19435233, 0.44518843, -0.0175227, 0.51211065, 0.5411712, 0.61696094, 0.084487654, nan, 0.031744156]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>Human Resources|\\nConflict Management|\\nPolici...</td>\n",
       "      <td>Dallas/Fort Worth Area</td>\n",
       "      <td>409</td>\n",
       "      <td>0.667733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>88</td>\n",
       "      <td>Human Resources Management Major</td>\n",
       "      <td>Milpitas, California</td>\n",
       "      <td>18</td>\n",
       "      <td>0.627603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>Human Resources Generalist at Loparex</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.616961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>Human Resources Generalist at Schwan's</td>\n",
       "      <td>Amerika Birleşik Devletleri</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.616961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>Human Resources Generalist at ScottMadden, Inc.</td>\n",
       "      <td>Raleigh-Durham, North Carolina Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.616961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>Human Resources Specialist at Luxottica</td>\n",
       "      <td>Greater New York City Area</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.586784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>Director Human Resources  at EY</td>\n",
       "      <td>Greater Atlanta Area</td>\n",
       "      <td>349</td>\n",
       "      <td>0.576593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>Aspiring Human Resources Manager, seeking inte...</td>\n",
       "      <td>Houston, Texas Area</td>\n",
       "      <td>7</td>\n",
       "      <td>0.568806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>Liberal Arts Major. Aspiring Human Resources A...</td>\n",
       "      <td>Baton Rouge, Louisiana Area</td>\n",
       "      <td>7</td>\n",
       "      <td>0.544728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Aspiring Human Resources Manager | Graduating ...</td>\n",
       "      <td>Cape Girardeau, Missouri</td>\n",
       "      <td>103</td>\n",
       "      <td>0.541171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          job_title  \\\n",
       "76    77  Human Resources|\\nConflict Management|\\nPolici...   \n",
       "87    88                   Human Resources Management Major   \n",
       "100  101              Human Resources Generalist at Loparex   \n",
       "77    78             Human Resources Generalist at Schwan's   \n",
       "70    71    Human Resources Generalist at ScottMadden, Inc.   \n",
       "67    68            Human Resources Specialist at Luxottica   \n",
       "88    89                    Director Human Resources  at EY   \n",
       "72    73  Aspiring Human Resources Manager, seeking inte...   \n",
       "78    79  Liberal Arts Major. Aspiring Human Resources A...   \n",
       "99   100  Aspiring Human Resources Manager | Graduating ...   \n",
       "\n",
       "                                location connection       fit  \n",
       "76                Dallas/Fort Worth Area        409  0.667733  \n",
       "87                  Milpitas, California         18  0.627603  \n",
       "100  Raleigh-Durham, North Carolina Area      500+   0.616961  \n",
       "77           Amerika Birleşik Devletleri      500+   0.616961  \n",
       "70   Raleigh-Durham, North Carolina Area      500+   0.616961  \n",
       "67            Greater New York City Area      500+   0.586784  \n",
       "88                  Greater Atlanta Area        349  0.576593  \n",
       "72                   Houston, Texas Area          7  0.568806  \n",
       "78           Baton Rouge, Louisiana Area          7  0.544728  \n",
       "99              Cape Girardeau, Missouri        103  0.541171  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query= \"Aspiring human resources\"\n",
    "\n",
    "\n",
    "\n",
    "sim_score=Top_n(Query, skipgram2,skip_gram_vectors,data1)\n",
    "sim_score.head(10)\n",
    "# data['fit']=sim_score\n",
    "# data=data.sort_values(by='fit', ascending=False)\n",
    "# data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #model_name='bert-base-nli-mean-tokens'\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model=SentenceTransformer(model_name)\n",
    "# # sentence_vecs=[model.encode(sentence) for sentence in data.job_title]\n",
    "# # sentence_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[, ct, bauer, college, business, graduate, mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[native, english, teacher, epik, english, prog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[aspiring, human, resource, professional]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[people, development, coordinator, ryan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[advisory, board, member, celal, bayar, univer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title\n",
       "0  [, ct, bauer, college, business, graduate, mag...\n",
       "1  [native, english, teacher, epik, english, prog...\n",
       "2          [aspiring, human, resource, professional]\n",
       "3           [people, development, coordinator, ryan]\n",
       "4  [advisory, board, member, celal, bayar, univer..."
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title=pd.DataFrame(data.job_title)\n",
    "job_title.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model_name=\"bert-base-uncased\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model=AutoModel.from_pretrained(model_name)\n",
    "#model = BertModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(sentences):\n",
    "    return tokenizer(sentences,padding=True, truncation=True,return_tensors='pt')  #here is pytorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(tokenized_sent):\n",
    "    with torch.no_grad():\n",
    "        outputs=bert_model(**tokenized_sent)   #unpacking tokenized_sent because this is a dictionary\n",
    "        embeddings=outputs.last_hidden_state.mean(dim=1)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(tokenized_sent):\n",
    "    with torch.no_grad():\n",
    "        outputs=bert_model(**tokenized_sent)   #unpacking tokenized_sent because this is a dictionary\n",
    "        embeddings=outputs.last_hidden_state.mean(dim=1)   #mean_pooling\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score_Cal(df,Query):                     \n",
    "    score=[]\n",
    "    for index,row in df.iterrows():\n",
    "        sentence=row['job_title']\n",
    "\n",
    "        #Tokenizing and computing embeddings for raw data\n",
    "        tokenized_sent=tokenize_sent(sentence)\n",
    "        sentence_embed=compute_embeddings(tokenized_sent)\n",
    "\n",
    "        #Tokenizing and computing embedding for input query_mean\n",
    "        tokenized_query=tokenize_sent(Query)\n",
    "        query_embed=compute_embeddings(tokenized_query)\n",
    "                                        \n",
    "        #Calculating cosine_similarity      \n",
    "        Similarity_score=cosine_similarity(sentence_embed,query_embed)\n",
    "        score.append(Similarity_score[0][0])\n",
    "         \n",
    "    df['fit']=score\n",
    "    df=df.sort_values(by='fit', ascending=False)\n",
    "    df=df.drop_duplicates(subset=['job_title'])\n",
    "\n",
    "    return df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "Query= \"Aspiring Human Resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>[student, humber, college, aspiring, human, re...</td>\n",
       "      <td>[kanada]</td>\n",
       "      <td>61</td>\n",
       "      <td>0.714606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>102</td>\n",
       "      <td>[business, intelligence, analytics, traveler]</td>\n",
       "      <td>[greater, new, york, city, area]</td>\n",
       "      <td>49</td>\n",
       "      <td>0.700047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>72</td>\n",
       "      <td>[business, management, major, aspiring, human,...</td>\n",
       "      <td>[monroe, louisiana, area]</td>\n",
       "      <td>5</td>\n",
       "      <td>0.700047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>[student, indiana, university, kokomo, busines...</td>\n",
       "      <td>[lafayette, indiana]</td>\n",
       "      <td>19</td>\n",
       "      <td>0.698432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>[director, human, resource, ey]</td>\n",
       "      <td>[greater, atlanta, area]</td>\n",
       "      <td>349</td>\n",
       "      <td>0.688109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>[student]</td>\n",
       "      <td>[houston, texas, area]</td>\n",
       "      <td>4</td>\n",
       "      <td>0.686118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>95</td>\n",
       "      <td>[student, westfield, state, university]</td>\n",
       "      <td>[bridgewater, massachusetts]</td>\n",
       "      <td>57</td>\n",
       "      <td>0.686118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>[student, chapman, university]</td>\n",
       "      <td>[lake, forest, california]</td>\n",
       "      <td>2</td>\n",
       "      <td>0.686118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>[people, development, coordinator, ryan]</td>\n",
       "      <td>[denton, texas]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.677051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>[aspiring, human, resource, manager, graduatin...</td>\n",
       "      <td>[cape, girardeau, missouri]</td>\n",
       "      <td>103</td>\n",
       "      <td>0.675043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>81</td>\n",
       "      <td>[senior, human, resource, business, partner, h...</td>\n",
       "      <td>[chattanooga, tennessee, area]</td>\n",
       "      <td>455</td>\n",
       "      <td>0.674481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>104</td>\n",
       "      <td>[director, administration, excellence, logging]</td>\n",
       "      <td>[katy, texas]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.669402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>[aspiring, human, resource, professional, ener...</td>\n",
       "      <td>[austin, texas, area]</td>\n",
       "      <td>174</td>\n",
       "      <td>0.664369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>[director, human, resource, north, america, gr...</td>\n",
       "      <td>[greater, grand, rapid, michigan, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.663378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>[junior, me, engineer, information, system]</td>\n",
       "      <td>[myrtle, beach, south, carolina, area]</td>\n",
       "      <td>52</td>\n",
       "      <td>0.654574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          job_title  \\\n",
       "38    39  [student, humber, college, aspiring, human, re...   \n",
       "101  102      [business, intelligence, analytics, traveler]   \n",
       "71    72  [business, management, major, aspiring, human,...   \n",
       "95    96  [student, indiana, university, kokomo, busines...   \n",
       "88    89                    [director, human, resource, ey]   \n",
       "97    98                                          [student]   \n",
       "94    95            [student, westfield, state, university]   \n",
       "40    41                     [student, chapman, university]   \n",
       "58    59           [people, development, coordinator, ryan]   \n",
       "99   100  [aspiring, human, resource, manager, graduatin...   \n",
       "80    81  [senior, human, resource, business, partner, h...   \n",
       "103  104    [director, administration, excellence, logging]   \n",
       "81    82  [aspiring, human, resource, professional, ener...   \n",
       "68    69  [director, human, resource, north, america, gr...   \n",
       "79    80        [junior, me, engineer, information, system]   \n",
       "\n",
       "                                    location connection       fit  \n",
       "38                                  [kanada]         61  0.714606  \n",
       "101         [greater, new, york, city, area]         49  0.700047  \n",
       "71                 [monroe, louisiana, area]          5  0.700047  \n",
       "95                      [lafayette, indiana]         19  0.698432  \n",
       "88                  [greater, atlanta, area]        349  0.688109  \n",
       "97                    [houston, texas, area]          4  0.686118  \n",
       "94              [bridgewater, massachusetts]         57  0.686118  \n",
       "40                [lake, forest, california]          2  0.686118  \n",
       "58                           [denton, texas]      500+   0.677051  \n",
       "99               [cape, girardeau, missouri]        103  0.675043  \n",
       "80            [chattanooga, tennessee, area]        455  0.674481  \n",
       "103                            [katy, texas]      500+   0.669402  \n",
       "81                     [austin, texas, area]        174  0.664369  \n",
       "68   [greater, grand, rapid, michigan, area]      500+   0.663378  \n",
       "79    [myrtle, beach, south, carolina, area]         52  0.654574  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity_score_Cal(data,Query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "sbert_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SBERT_Similarity_score_Cal(df,Query,model):                     \n",
    "    score=[]\n",
    "    for index,row in df.iterrows():\n",
    "        sentence=row['job_title']\n",
    "\n",
    "        #Tokenizing and computing embeddings for raw data\n",
    "        sentence_embed=model.encode(sentence)\n",
    "\n",
    "        #Tokenizing and computing embedding for input query_mean\n",
    "        query_embed=model.encode(Query)\n",
    "        query_embed=query_embed.reshape(1,-1)\n",
    "                               \n",
    "        #Calculating cosine_similarity      \n",
    "        Similarity_score=cosine_similarity(sentence_embed,query_embed)\n",
    "        score.append(Similarity_score[0][0])\n",
    "    \n",
    "    print('this is score scliced: ',score)    \n",
    "    df['fit']=score\n",
    "    df=df.sort_values(by='fit', ascending=False)\n",
    "    df=df.drop_duplicates(subset=['job_title'])\n",
    "    return df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is score scliced:  [0.1657722, 0.15741763, 0.49409574, 0.20827627, 0.19119422, 0.49409574, 0.1856448, 0.4268193, 0.1856448, 0.36639318, 0.18564469, 0.059747767, 0.4529913, 0.1657722, 0.1657722, 0.15741763, 0.49409574, 0.20827627, 0.1657722, 0.15741763, 0.49409574, 0.20827627, 0.19119422, 0.49409574, 0.1856448, 0.4268193, 0.49409574, 0.3663931, 0.49409574, 0.3663931, 0.1657722, 0.15741763, 0.49409574, 0.20827627, 0.19119422, 0.49409574, 0.1856448, 0.4268193, 0.1856448, 0.36639318, 0.18564469, 0.059747767, 0.4529913, 0.1657722, 0.15741763, 0.49409574, 0.20827627, 0.19119422, 0.49409574, 0.1856448, 0.4268193, 0.1856448, 0.36639318, 0.18564469, 0.059747767, 0.4529913, 0.1657722, 0.49409574, 0.20827627, 0.49409574, 0.4268193, 0.36639318, 0.18564469, 0.059747767, 0.4529913, 0.27511165, 0.45299122, 0.4529913, 0.2436051, 0.14023048, 0.4529913, 0.11087361, 0.49409574, 0.4529913, 0.06620973, 0.49409574, 0.4529913, 0.4529913, 0.0007651951, 0.097496495, 0.111794114, 0.49409574, 0.42681938, 0.4529912, 0.08275923, 0.22491515, 0.16946733, 0.4529912, 0.2436051, 0.22513959, 0.1742651, 0.3663931, 0.13508743, 0.3663931, 0.18564469, 0.18564478, 0.49409574, 0.18564476, 0.3663931, 0.49409574, 0.4529913, 0.11087361, -0.006290136, 0.24360506]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>job_title</th>\n",
       "      <th>location</th>\n",
       "      <th>connection</th>\n",
       "      <th>fit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>[aspiring, human, resource, specialist]</td>\n",
       "      <td>[greater, new, york, city, area]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>[aspiring, human, resource, management, studen...</td>\n",
       "      <td>[houston, texas, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>[aspiring, human, resource, professional]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>44</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>82</td>\n",
       "      <td>[aspiring, human, resource, professional, ener...</td>\n",
       "      <td>[austin, texas, area]</td>\n",
       "      <td>174</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>73</td>\n",
       "      <td>[aspiring, human, resource, manager, seeking, ...</td>\n",
       "      <td>[houston, texas, area]</td>\n",
       "      <td>7</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>[aspiring, human, resource, professional, pass...</td>\n",
       "      <td>[new, york, new, york]</td>\n",
       "      <td>212</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>[aspiring, human, resource, manager, graduatin...</td>\n",
       "      <td>[cape, girardeau, missouri]</td>\n",
       "      <td>103</td>\n",
       "      <td>0.494096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>[human, resource, coordinator, intercontinenta...</td>\n",
       "      <td>[atlanta, georgia]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>[human, resource, specialist, luxottica]</td>\n",
       "      <td>[greater, new, york, city, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>[human, resource, generalist, scottmadden, inc]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>74</td>\n",
       "      <td>[human, resource, professional]</td>\n",
       "      <td>[greater, boston, area]</td>\n",
       "      <td>16</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>[human, resource, conflict, management, policy...</td>\n",
       "      <td>[dallasfort, worth, area]</td>\n",
       "      <td>409</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>101</td>\n",
       "      <td>[human, resource, generalist, loparex]</td>\n",
       "      <td>[raleighdurham, north, carolina, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>[human, resource, generalist, schwan, s]</td>\n",
       "      <td>[amerika, birleşik, devletleri]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>[human, resource, staffing, recruiting, profes...</td>\n",
       "      <td>[jackson, mississippi, area]</td>\n",
       "      <td>500+</td>\n",
       "      <td>0.452991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                          job_title  \\\n",
       "48    49            [aspiring, human, resource, specialist]   \n",
       "26    27  [aspiring, human, resource, management, studen...   \n",
       "20    21          [aspiring, human, resource, professional]   \n",
       "81    82  [aspiring, human, resource, professional, ener...   \n",
       "72    73  [aspiring, human, resource, manager, seeking, ...   \n",
       "75    76  [aspiring, human, resource, professional, pass...   \n",
       "99   100  [aspiring, human, resource, manager, graduatin...   \n",
       "42    43  [human, resource, coordinator, intercontinenta...   \n",
       "67    68           [human, resource, specialist, luxottica]   \n",
       "70    71    [human, resource, generalist, scottmadden, inc]   \n",
       "73    74                    [human, resource, professional]   \n",
       "76    77  [human, resource, conflict, management, policy...   \n",
       "100  101             [human, resource, generalist, loparex]   \n",
       "77    78           [human, resource, generalist, schwan, s]   \n",
       "66    67  [human, resource, staffing, recruiting, profes...   \n",
       "\n",
       "                                   location connection       fit  \n",
       "48         [greater, new, york, city, area]          1  0.494096  \n",
       "26                   [houston, texas, area]      500+   0.494096  \n",
       "20   [raleighdurham, north, carolina, area]         44  0.494096  \n",
       "81                    [austin, texas, area]        174  0.494096  \n",
       "72                   [houston, texas, area]          7  0.494096  \n",
       "75                   [new, york, new, york]        212  0.494096  \n",
       "99              [cape, girardeau, missouri]        103  0.494096  \n",
       "42                       [atlanta, georgia]      500+   0.452991  \n",
       "67         [greater, new, york, city, area]      500+   0.452991  \n",
       "70   [raleighdurham, north, carolina, area]      500+   0.452991  \n",
       "73                  [greater, boston, area]         16  0.452991  \n",
       "76                [dallasfort, worth, area]        409  0.452991  \n",
       "100  [raleighdurham, north, carolina, area]      500+   0.452991  \n",
       "77          [amerika, birleşik, devletleri]      500+   0.452991  \n",
       "66             [jackson, mississippi, area]      500+   0.452991  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Query= \"Aspiring Human Resources\"\n",
    "\n",
    "SBERT_Similarity_score_Cal(data,Query,sbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='sentence-transformers/bert-base-nli-mean-tokens'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "model=AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens={'input_ids':[],'attention_mask':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data1['job_title']:\n",
    "    new_tokens=tokenizer.encode_plus(sentence,max_length=128,truncation=True,padding='max_length',return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids']=torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask']=torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids']=torch.squeeze(tokens['input_ids'])\n",
    "tokens['attention_mask']=torch.squeeze(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128, 768])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs=model(**tokens)\n",
    "embeddings=outputs.last_hidden_state\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiplying every value within last hidden state tensor by 0 where we shouldnt have a real token\n",
    "\n",
    "attention=tokens['attention_mask']\n",
    "attention.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128, 768])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.unsqueeze(-1).expand(embeddings.shape).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 128, 768])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting from integer to float\n",
    "mask=attention.unsqueeze(-1).expand(embeddings.shape).float()\n",
    "\n",
    "\n",
    "mask_embed= embeddings*mask\n",
    "mask_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 768])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed=torch.sum(mask_embed,1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 768])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts=torch.clamp(mask.sum(1),min=1e-9)\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([104, 768])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled=summed/counts\n",
    "mean_pooled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled=mean_pooled.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embedding_shape:  torch.Size([1, 128, 768])\n",
      "query_attention shape:  torch.Size([128])\n",
      "query_mask_embedding_shape:  torch.Size([1, 128, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_mean_pooled shape:  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "Query= \"Aspiring Human Resources\"\n",
    "\n",
    "\n",
    "query_token={'input_ids':[],'attention_mask':[]}\n",
    "\n",
    "query_tokens=tokenizer.encode_plus(Query,max_length=128,padding='max_length',truncation=True,return_tensors='pt')\n",
    "query_token['input_ids'].append(query_tokens['input_ids'])\n",
    "query_token['attention_mask'].append(query_tokens['attention_mask'])\n",
    "\n",
    "query_token['input_ids']=torch.stack(query_token['input_ids'])\n",
    "query_token['attention_mask']=torch.stack(query_token['attention_mask'])\n",
    "\n",
    "query_token['input_ids']=torch.squeeze(query_token['input_ids'])\n",
    "query_token['attention_mask']=torch.squeeze(query_token['attention_mask'])\n",
    "\n",
    "query_outputs=model(**query_tokens)\n",
    "query_embeddings=query_outputs.last_hidden_state\n",
    "print('query_embedding_shape: ',query_embeddings.shape)\n",
    "\n",
    "query_attention=query_token['attention_mask']\n",
    "print('query_attention shape: ',query_attention.shape)\n",
    "\n",
    "\n",
    "#Converting from integer to float\n",
    "query_mask=query_attention.unsqueeze(-1).expand(query_embeddings.shape).float()\n",
    "\n",
    "\n",
    "query_mask_embed= query_embeddings*query_mask\n",
    "print('query_mask_embedding_shape: ',query_mask_embed.shape)\n",
    "\n",
    "\n",
    "q_summed=torch.sum(query_mask_embed,1)\n",
    "q_summed.shape\n",
    "\n",
    "q_counts=torch.clamp(query_mask.sum(1),min=1e-9)\n",
    "q_counts.shape\n",
    "\n",
    "q_mean_pooled=q_summed/q_counts\n",
    "print('query_mean_pooled shape: ',q_mean_pooled.shape)\n",
    "\n",
    "q_mean_pooled=q_mean_pooled.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49784946, 0.37227935, 0.92721367, 0.7058313 , 0.46320167,\n",
       "        0.9426098 , 0.7215088 , 0.6806129 , 0.7215088 , 0.764407  ,\n",
       "        0.4487623 , 0.29949868, 0.39719856, 0.49784946, 0.49784946,\n",
       "        0.37227935, 0.92721367, 0.7058313 , 0.49784946, 0.37227935,\n",
       "        0.92721367, 0.7058313 , 0.46320167, 0.9426098 , 0.7215088 ,\n",
       "        0.6806129 , 0.67100716, 0.8337132 , 0.67100716, 0.8337132 ,\n",
       "        0.49784946, 0.37227935, 0.92721367, 0.7058313 , 0.46320167,\n",
       "        0.9426098 , 0.7215088 , 0.6806129 , 0.7215088 , 0.764407  ,\n",
       "        0.4487623 , 0.29949868, 0.39719856, 0.49784946, 0.37227935,\n",
       "        0.92721367, 0.7058313 , 0.46320167, 0.9426098 , 0.7215088 ,\n",
       "        0.6806129 , 0.7215088 , 0.764407  , 0.4487623 , 0.29949868,\n",
       "        0.39719856, 0.49784946, 0.92721367, 0.7058313 , 0.9426098 ,\n",
       "        0.6806129 , 0.764407  , 0.4487623 , 0.29949868, 0.39719856,\n",
       "        0.7007383 , 0.75128305, 0.6953852 , 0.4491577 , 0.42338023,\n",
       "        0.54158014, 0.65605044, 0.7402718 , 0.78272295, 0.54420346,\n",
       "        0.69621825, 0.61034715, 0.5381652 , 0.5975434 , 0.56517774,\n",
       "        0.48132113, 0.80302584, 0.37163407, 0.5379616 , 0.259422  ,\n",
       "        0.50971293, 0.08629327, 0.683863  , 0.7035969 , 0.437305  ,\n",
       "        0.38351032, 0.62829995, 0.16493395, 0.6705387 , 0.49145722,\n",
       "        0.25283498, 0.92721367, 0.65468884, 0.8477609 , 0.5008614 ,\n",
       "        0.60929966, 0.45511636, 0.61895883, 0.48834157]], dtype=float32)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(q_mean_pooled,mean_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "\n",
    "def get_sentence_similarity(tokenizer,model,s1,s2):\n",
    "\n",
    "    s1 = tokenizer.encode(s1)  \n",
    "    s2 = tokenizer.encode(s2)\n",
    "\n",
    "    print(\"1 len(s1) s1\",len(s1),s1) # prints length of tokens - input_ids 8 [101, 7592...\n",
    "    print(\"1 len(s2) s2\",len(s2),s2)\n",
    "    s1 = torch.tensor(s1)\n",
    "    #print(\"2\",s1) # prints tensor([ 101, 7592, ...\n",
    "    s1 = s1.unsqueeze(0) # add an extra dimension, why ? the model needs to be fed in batches, we give a dummy batch 1\n",
    "    #print(\"3\",s1) # prints tensor([[ 101, 7592, \n",
    "    s2 = torch.tensor(s2).unsqueeze(0)\n",
    "\n",
    "    # Pass it to the model for inference\n",
    "    with torch.no_grad():\n",
    "        output_1 = model(s1)\n",
    "        output_2 = model(s2)\n",
    "\n",
    "    logits_s1 = output_1[0]  # The last hidden-state is the first element of the output tuple\n",
    "    logits_s2 = output_2[0].detach()\n",
    "    #print(\"logits_s1 before detach\",logits_s1) # prints  tensor([[[-0.1162,  0.2388, ...-0.2128]]], grad_fn=<NativeLayerNormBackward0>)\n",
    "    logits_s1 = logits_s1.detach() # to remove the last part we call detach\n",
    "\n",
    "    print(\"logits_s1.shape\",logits_s1.shape ) # prints ([1, <length of tokens>, 768]) - Each token is rep by a 768 row vector for the base Bert Model!\n",
    "    print(\"logits_s2.shape\",logits_s2.shape ) # 1 the dummy batch dimension we added to the model by un-squeeze\n",
    "    logits_s1 = torch.squeeze(logits_s1) #lets remove the batch dimension by squeeze\n",
    "    logits_s2 = torch.squeeze(logits_s2)\n",
    "    print(\"logits_s1.shape\",logits_s1.shape ) # prints ([<length of tokens>, 768]) say torch.Size([8, 768])\n",
    "    print(\"logits_s2.shape\",logits_s2.shape )\n",
    "    a = logits_s1.reshape(1,logits_s1.numel()) # we lay the vector flat make it 1, **768 via reshape; numel is number of elements\n",
    "    b = logits_s2.reshape(1,logits_s2.numel())\n",
    "    print(\"a.shape\",a.shape ) # torch.Size([1, 6144])\n",
    "    print(\"b.shape\",b.shape ) # the shape will be 1, 768* no of tokens in b sentence - need not be similar\n",
    "\n",
    "    # we can  mean over the rows to give it better similarity - but that is giving poor output\n",
    "    # a = sentence_vector_1.mean(axis=1) this is giving cosine similarity as 1\n",
    "    # b = sentence_vector_2.mean(axis=1)\n",
    "    #cos_sim = F.cosine_similarity(a.reshape(1,-1),b.reshape(1,-1), dim=1)\n",
    "\n",
    "    # so we pad the tensors to be same shape\n",
    "    if  a.shape[1] <  b.shape[1]:\n",
    "        pad_size = (0, b.shape[1] - a.shape[1]) \n",
    "        a = torch.nn.functional.pad(a, pad_size, mode='constant', value=0)\n",
    "    else:\n",
    "        pad_size = (0, a.shape[1] - b.shape[1]) \n",
    "        b = torch.nn.functional.pad(b, pad_size, mode='constant', value=0)\n",
    "\n",
    "    print(\"After padding\")\n",
    "    print(\"a.shape\",a.shape ) # 1,N\n",
    "    print(\"b.shape\",b.shape ) # 1, N\n",
    "\n",
    "\n",
    "    # Calculate the cosine similarity\n",
    "    cos_sim = cosine_similarity(a,b)\n",
    "    #print(\"got cosine similarity\",cos_sim) # output [[0.80432487]]\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 384)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 len(s1) s1 5 [101, 2198, 7459, 6077, 102]\n",
      "1 len(s2) s2 6 [101, 1045, 2031, 2908, 4689, 102]\n",
      "logits_s1.shape torch.Size([1, 5, 384])\n",
      "logits_s2.shape torch.Size([1, 6, 384])\n",
      "logits_s1.shape torch.Size([5, 384])\n",
      "logits_s2.shape torch.Size([6, 384])\n",
      "a.shape torch.Size([1, 1920])\n",
      "b.shape torch.Size([1, 2304])\n",
      "After padding\n",
      "a.shape torch.Size([1, 2304])\n",
      "b.shape torch.Size([1, 2304])\n"
     ]
    }
   ],
   "source": [
    "ss1 = \"John loves dogs\" \n",
    "ss2 = \"I have gone crazy\"\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = BertModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model.eval()\n",
    "cos_sim = get_sentence_similarity(tokenizer,model,ss1,ss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row_vector_reshaped = fast_text_vectors\n",
    "# print(\"row_vec_shape 1: \", row_vector_reshaped.shape)\n",
    "# for i in range(len(row_vector_reshaped)):\n",
    "#     row_vector = row_vector_reshaped[i]\n",
    "#     print(\"row_vec_shape\", row_vector.shape)\n",
    "#     print(row_vector)\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
